---
title: 'R : Selecting Informative Predictors Using Simulated Annealing and Genetic Algorithms'
author: "John Pauline Pineda"
date: "December 22, 2022"
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
    highlight: tango
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=15, fig.height=10)
```
# **1. Table of Contents**
|
| This document implements simulated annealing and genetic algorithms for selecting informative predictors using various helpful packages in <mark style="background-color: #CCECFF">**R**</mark>.  
|
| Feature selection is the process of reducing the number of input variables by eliminating redundant or irrelevant features and narrowing down the set of features to those most relevant to the machine learning model, resulting to simpler explainable models, shorter training times due to a more precise subset of features, reduction of variance, increase in precision estimates and avoidance against the curse of high dimensionality. The algorithms applied in this study (mostly contained in the <mark style="background-color: #CCECFF">**caret**</mark> package) are wrapper feature selection methods which attempt to consider the selection of a set of features as a search problem, whereby their quality is assessed with the preparation, evaluation, and comparison of a combination of features to other combinations of features. 
|
##  1.1 Sample Data
|
| The <mark style="background-color: #EEEEEE;color: #FF0000">**AlzheimerDisease**</mark>  dataset from the  <mark style="background-color: #CCECFF">**AppliedPredictiveModeling**</mark> package was used for this illustrated example.
|
| Preliminary dataset assessment:
|
| **[A]** 333 rows (observations)
|      **[A.1]** Train Set = 267 observations
|      **[A.2]** Test Set = 66 observations
| 
| **[B]** 128 columns (variables)
|      **[B.1]** 1/128 response = <span style="color: #FF0000">Class</span> variable (factor)
|             **[B.1.1]** Levels = <span style="color: #FF0000">Class=Control</span> < <span style="color: #FF0000">Class=Impaired</span>
|      **[B.2]** 127/128 predictors = All remaining variables (3/127 factor + 124/127 numeric)
|     
| 
```{r section_1.1, warning=FALSE, message=FALSE}
##################################
# Loading R libraries
##################################
library(AppliedPredictiveModeling)
library(caret)
library(rpart)
library(lattice)
library(dplyr)
library(tidyr)
library(moments)
library(skimr)
library(RANN)
library(pls)
library(corrplot)
library(tidyverse)
library(lares)
library(DMwR)
library(gridExtra)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(stats)
library(nnet)
library(elasticnet)
library(earth)
library(party)
library(kernlab)
library(randomForest)
library(Cubist)
library(pROC)
library(mda)
library(klaR)
library(pamr)
library(MLmetrics)

##################################
# Loading source and
# formulating the train set
##################################
data(AlzheimerDisease)
Alzheimer <- predictors
Alzheimer$Class <- diagnosis

##################################
# Decomposing the Genotype factor
# into binary dummy variables
##################################

## Decompose the genotype factor into binary dummy variables

Alzheimer$E2 <- Alzheimer$E3 <- Alzheimer$E4 <- 0
Alzheimer$E2[grepl("2", Alzheimer$Genotype)] <- 1
Alzheimer$E3[grepl("3", Alzheimer$Genotype)] <- 1
Alzheimer$E4[grepl("4", Alzheimer$Genotype)] <- 1
Alzheimer_Original <- Alzheimer

##################################
# Removing baseline predictors
##################################
Alzheimer <- Alzheimer[,!(names(Alzheimer) %in% c("Genotype", "age", "tau", "p_tau", "Ab_42", "male"))]

##################################
# Partitoning the data into
# train and test sets
##################################
set.seed(12345678)
Alzheimer_Train_Index <- createDataPartition(Alzheimer$Class,p=0.8)[[1]]
Alzheimer_Train <- Alzheimer[ Alzheimer_Train_Index, ]
Alzheimer_Test  <- Alzheimer[-Alzheimer_Train_Index, ]

##################################
# Performing a general exploration of the train set
##################################
dim(Alzheimer_Train)
str(Alzheimer_Train)
summary(Alzheimer_Train)

##################################
# Performing a general exploration of the test set
##################################
dim(Alzheimer_Test)
str(Alzheimer_Test)
summary(Alzheimer_Test)

##################################
# Formulating a data type assessment summary
##################################
PDA <- Alzheimer_Train
(PDA.Summary <- data.frame(
  Column.Index=c(1:length(names(PDA))),
  Column.Name= names(PDA), 
  Column.Type=sapply(PDA, function(x) class(x)), 
  row.names=NULL)
)
```
##  1.2 Data Quality Assessment
|
| Data quality assessment:
|
| **[A]** No missing observations noted for any variable.
|
| **[B]** Low variance observed for 2 variables with First.Second.Mode.Ratio>5.
|      **[B.1]** <span style="color: #FF0000">E2</span> variable (factor)
|      **[B.2]** <span style="color: #FF0000">E3</span> variable (factor)
|
| **[C]** No low variance observed for any variable with Unique.Count.Ratio<0.01.
|
| **[D]** No high skewness observed for any variable with Skewness>3 or Skewness<(-3).
|
```{r section_1.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DQA <- Alzheimer_Train

##################################
# Formulating an overall data quality assessment summary
##################################
(DQA.Summary <- data.frame(
  Column.Index=c(1:length(names(DQA))),
  Column.Name= names(DQA), 
  Column.Type=sapply(DQA, function(x) class(x)), 
  Row.Count=sapply(DQA, function(x) nrow(DQA)),
  NA.Count=sapply(DQA,function(x)sum(is.na(x))),
  Fill.Rate=sapply(DQA,function(x)format(round((sum(!is.na(x))/nrow(DQA)),3),nsmall=3)),
  row.names=NULL)
)

##################################
# Listing all predictors
##################################
DQA.Predictors <- DQA[,!names(DQA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DQA.Predictors.Numeric <- DQA.Predictors[,!names(DQA.Predictors) %in% c("E2","E3","E4")]
DQA.Predictors.Numeric <- as.data.frame(sapply(DQA.Predictors.Numeric,function(x) as.numeric(x)))

if (length(names(DQA.Predictors.Numeric))>0) {
    print(paste0("There are ",
               (length(names(DQA.Predictors.Numeric))),
               " numeric predictor variable(s)."))
} else {
  print("There are no numeric predictor variables.")
}

##################################
# Listing all factor predictors
##################################
DQA.Predictors.Factor <- DQA.Predictors[,names(DQA.Predictors) %in% c("E2","E3","E4")]
DQA.Predictors.Factor <- as.data.frame(sapply(DQA.Predictors.Factor,function(x) as.factor(x)))

if (length(names(DQA.Predictors.Factor))>0) {
    print(paste0("There are ",
               (length(names(DQA.Predictors.Factor))),
               " factor predictor variable(s)."))
} else {
  print("There are no factor predictor variables.")
}

##################################
# Formulating a data quality assessment summary for factor predictors
##################################
if (length(names(DQA.Predictors.Factor))>0) {
  
  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = x[!(x %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return("x"),
           return(usm[tabsm == max(tabsm)]))
  }
  
  (DQA.Predictors.Factor.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Factor), 
  Column.Type=sapply(DQA.Predictors.Factor, function(x) class(x)), 
  Unique.Count=sapply(DQA.Predictors.Factor, function(x) length(unique(x))),
  First.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(FirstModes(x)[1])),
  Second.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(SecondModes(x)[1])),
  First.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  Unique.Count.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Factor)),3), nsmall=3)),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  row.names=NULL)
  )
  
} 

##################################
# Formulating a data quality assessment summary for numeric predictors
##################################
if (length(names(DQA.Predictors.Numeric))>0) {
  
  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = na.omit(x)[!(na.omit(x) %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return(0.00001),
           return(usm[tabsm == max(tabsm)]))
  }
  
  (DQA.Predictors.Numeric.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Numeric), 
  Column.Type=sapply(DQA.Predictors.Numeric, function(x) class(x)), 
  Unique.Count=sapply(DQA.Predictors.Numeric, function(x) length(unique(x))),
  Unique.Count.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Numeric)),3), nsmall=3)),
  First.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((FirstModes(x)[1]),3),nsmall=3)),
  Second.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((SecondModes(x)[1]),3),nsmall=3)),
  First.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  Minimum=sapply(DQA.Predictors.Numeric, function(x) format(round(min(x,na.rm = TRUE),3), nsmall=3)),
  Mean=sapply(DQA.Predictors.Numeric, function(x) format(round(mean(x,na.rm = TRUE),3), nsmall=3)),
  Median=sapply(DQA.Predictors.Numeric, function(x) format(round(median(x,na.rm = TRUE),3), nsmall=3)),
  Maximum=sapply(DQA.Predictors.Numeric, function(x) format(round(max(x,na.rm = TRUE),3), nsmall=3)),
  Skewness=sapply(DQA.Predictors.Numeric, function(x) format(round(skewness(x,na.rm = TRUE),3), nsmall=3)),
  Kurtosis=sapply(DQA.Predictors.Numeric, function(x) format(round(kurtosis(x,na.rm = TRUE),3), nsmall=3)),
  Percentile25th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.25,na.rm = TRUE),3), nsmall=3)),
  Percentile75th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.75,na.rm = TRUE),3), nsmall=3)),
  row.names=NULL)
  )  
  
}

##################################
# Identifying potential data quality issues
##################################

##################################
# Checking for missing observations
##################################
if ((nrow(DQA.Summary[DQA.Summary$NA.Count>0,]))>0){
  print(paste0("Missing observations noted for ",
               (nrow(DQA.Summary[DQA.Summary$NA.Count>0,])),
               " variable(s) with NA.Count>0 and Fill.Rate<1.0."))
  DQA.Summary[DQA.Summary$NA.Count>0,]
} else {
  print("No missing observations noted.")
}

##################################
# Checking for zero or near-zero variance predictors
##################################
if (length(names(DQA.Predictors.Factor))==0) {
  print("No factor predictors noted.")
} else if (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])),
               " factor variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance factor predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])),
               " numeric variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance numeric predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])),
               " numeric variable(s) with Unique.Count.Ratio<0.01."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,]
} else {
  print("No low variance numeric predictors due to low unique count ratio noted.")
}

##################################
# Checking for skewed predictors
##################################
if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])>0){
  print(paste0("High skewness observed for ",
  (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])),
  " numeric variable(s) with Skewness>3 or Skewness<(-3)."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                 as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),]
} else {
  print("No skewed numeric predictors noted.")
}

```
##  1.3 Data Preprocessing

###  1.3.1 Outlier
|
| Outlier data assessment:
|
| **[A]** Few outliers noted for most variables  with the numeric data visualized through a boxplot including observations classified as suspected outliers using the IQR criterion. The IQR criterion means that all observations above the (75th percentile + 1.5 x IQR) or below the (25th percentile - 1.5 x IQR) are suspected outliers, where IQR is the difference between the third quartile (75th percentile) and first quartile (25th percentile). Outlier treatment for numerical stability remains optional depending on potential model requirements for the subsequent steps. 6 variables were observed with at least 10 outliers listed as follows:
|      **[A.1]** <span style="color: #FF0000">Apolipoprotein_CI</span> variable (10 outliers detected)
|      **[A.2]** <span style="color: #FF0000">Cortisol</span> variable (11 outliers detected)
|      **[A.3]** <span style="color: #FF0000">IL_17E</span> variable (11 outliers detected)
|      **[A.4]** <span style="color: #FF0000">IL6</span> variable (19 outliers detected)
|      **[A.5]** <span style="color: #FF0000">MCP_2</span> variable (21 outliers detected)
|      **[A.6]** <span style="color: #FF0000">Prostatic_Acid_Phospatase</span> variable (10 outliers detected)
|
```{r section_1.3.1, warning=FALSE, message=FALSE, fig.width=15, fig.height=3}
##################################
# Loading dataset
##################################
DPA <- Alzheimer_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,!names(DPA.Predictors) %in% c("E2","E3","E4")]
DPA.Predictors.Numeric <- as.data.frame(sapply(DPA.Predictors.Numeric,function(x) as.numeric(x)))

##################################
# Identifying outliers for the numeric predictors
##################################
OutlierCountList <- c()

for (i in 1:ncol(DPA.Predictors.Numeric)) {
  Outliers <- boxplot.stats(DPA.Predictors.Numeric[,i])$out
  OutlierCount <- length(Outliers)
  OutlierCountList <- append(OutlierCountList,OutlierCount)
  OutlierIndices <- which(DPA.Predictors.Numeric[,i] %in% c(Outliers))
  boxplot(DPA.Predictors.Numeric[,i], 
          ylab = names(DPA.Predictors.Numeric)[i], 
          main = names(DPA.Predictors.Numeric)[i],
          horizontal=TRUE)
  mtext(paste0(OutlierCount, " Outlier(s) Detected"))
}

OutlierCountSummary <- as.data.frame(cbind(names(DPA.Predictors.Numeric),(OutlierCountList)))
names(OutlierCountSummary) <- c("NumericPredictors","OutlierCount")
OutlierCountSummary$OutlierCount <- as.numeric(as.character(OutlierCountSummary$OutlierCount))
NumericPredictorWithOutlierCount <- nrow(OutlierCountSummary[OutlierCountSummary$OutlierCount>0,])
print(paste0(NumericPredictorWithOutlierCount, " numeric variable(s) were noted with outlier(s)." ))

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA.Predictors.Numeric))

###################################
# Verifying the data dimensions
###################################
dim(DPA.Predictors.Numeric)

```

###  1.3.2 Zero and Near-Zero Variance
|
| Zero and near-zero variance data assessment:
|
| **[A]** Low variance noted for 2 variables from the previous data quality assessment using a lower threshold.
|
| **[B]** No low variance noted for any variable using a preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package. The <span style="color: #0000FF">nearZeroVar</span> method using both the <span style="color: #0000FF">freqCut</span> and <span style="color: #0000FF">uniqueCut</span> criteria set at 95/5 and 10, respectively, were applied on the dataset.
|
```{r section_1.3.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- DPA.Predictors

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA))

##################################
# Identifying columns with low variance
###################################
DPA_LowVariance <- nearZeroVar(DPA,
                               freqCut = 95/5,
                               uniqueCut = 10,
                               saveMetrics= TRUE)
(DPA_LowVariance[DPA_LowVariance$nzv,])

if ((nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))==0){
  
  print("No low variance predictors noted.")
  
} else {

  print(paste0("Low variance observed for ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s) with First.Second.Mode.Ratio>4 and Unique.Count.Ratio<0.10."))
  
  DPA_LowVarianceForRemoval <- (nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))
  
  print(paste0("Low variance can be resolved by removing ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LowVarianceForRemoval) {
  DPA_LowVarianceRemovedVariable <- rownames(DPA_LowVariance[DPA_LowVariance$nzv,])[j]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LowVarianceRemovedVariable))
  }
  
  DPA %>%
  skim() %>%
  dplyr::filter(skim_variable %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,]))

  ##################################
  # Filtering out columns with low variance
  #################################
  DPA_ExcludedLowVariance <- DPA[,!names(DPA) %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,])]
  
  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedLowVariance_Skimmed <- skim(DPA_ExcludedLowVariance))
}

```

###  1.3.3 Collinearity
|
| High collinearity data assessment:
|
| **[A]** No high correlation > 95% were noted for any variable pair as confirmed using the preprocessing summaries from the <mark style="background-color: #CCECFF">**caret**</mark> and <mark style="background-color: #CCECFF">**lares**</mark> packages.
|
```{r section_1.3.3, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Alzheimer_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,!names(DPA.Predictors) %in% c("E2","E3","E4")]
DPA.Predictors.Numeric <- as.data.frame(sapply(DPA.Predictors.Numeric,function(x) as.numeric(x)))

##################################
# Visualizing pairwise correlation between predictors
##################################
DPA_CorrelationTest <- cor.mtest(DPA.Predictors.Numeric,
                       method = "pearson",
                       conf.level = .95)

corrplot(cor(DPA.Predictors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"), 
         method = "circle",
         type = "upper", 
         order = "original", 
         tl.col = "black", 
         tl.cex = 0.75,
         tl.srt = 90, 
         sig.level = 0.05, 
         p.mat = DPA_CorrelationTest$p,
         insig = "blank")



##################################
# Identifying the highly correlated variables
##################################
DPA_Correlation <-  cor(DPA.Predictors.Numeric, 
                        method = "pearson",
                        use="pairwise.complete.obs")
(DPA_HighlyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)]) > 0.95))

if (DPA_HighlyCorrelatedCount == 0) {
  print("No highly correlated predictors noted.")
} else {
  print(paste0("High correlation observed for ",
               (DPA_HighlyCorrelatedCount),
               " pairs of numeric variable(s) with Correlation.Coefficient>0.95."))
  
  (DPA_HighlyCorrelatedPairs <- corr_cross(DPA.Predictors.Numeric,
  max_pvalue = 0.05, 
  top = DPA_HighlyCorrelatedCount,
  rm.na = TRUE,
  grid = FALSE
))
  
}


if (DPA_HighlyCorrelatedCount > 0) {
  DPA_HighlyCorrelated <- findCorrelation(DPA_Correlation, cutoff = 0.95)
  
  (DPA_HighlyCorrelatedForRemoval <- length(DPA_HighlyCorrelated))
  
  print(paste0("High correlation can be resolved by removing ",
               (DPA_HighlyCorrelatedForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_HighlyCorrelatedForRemoval) {
  DPA_HighlyCorrelatedRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_HighlyCorrelated[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_HighlyCorrelatedRemovedVariable))
  }
  
  ##################################
  # Filtering out columns with high correlation
  #################################
  DPA_ExcludedHighCorrelation <- DPA[,-DPA_HighlyCorrelated]
  
  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedHighCorrelation_Skimmed <- skim(DPA_ExcludedHighCorrelation))

}

```

###  1.3.4 Linear Dependencies
|
| Linear dependency data assessment:
|
| **[A]** No linear dependencies noted for any subset of variables using the preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package applying the <span style="color: #0000FF">findLinearCombos</span> method which utilizes the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist). 
|
```{r section_1.3.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Alzheimer_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,!names(DPA.Predictors) %in% c("E2","E3","E4")]
DPA.Predictors.Numeric <- as.data.frame(sapply(DPA.Predictors.Numeric,function(x) as.numeric(x)))

##################################
# Identifying the linearly dependent variables
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)

(DPA_LinearlyDependentCount <- length(DPA_LinearlyDependent$linearCombos))

if (DPA_LinearlyDependentCount == 0) {
  print("No linearly dependent predictors noted.")
} else {
  print(paste0("Linear dependency observed for ",
               (DPA_LinearlyDependentCount),
               " subset(s) of numeric variable(s)."))
  
  for (i in 1:DPA_LinearlyDependentCount) {
    DPA_LinearlyDependentSubset <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$linearCombos[[i]]]
    print(paste0("Linear dependent variable(s) for subset ",
                 i,
                 " include: ",
                 DPA_LinearlyDependentSubset))
  }
  
}

##################################
# Identifying the linearly dependent variables for removal
##################################

if (DPA_LinearlyDependentCount > 0) {
  DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)
  
  DPA_LinearlyDependentForRemoval <- length(DPA_LinearlyDependent$remove)
  
  print(paste0("Linear dependency can be resolved by removing ",
               (DPA_LinearlyDependentForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LinearlyDependentForRemoval) {
  DPA_LinearlyDependentRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$remove[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LinearlyDependentRemovedVariable))
  }
  
  ##################################
  # Filtering out columns with linear dependency
  #################################
  DPA_ExcludedLinearlyDependent <- DPA[,-DPA_LinearlyDependent$remove]
  
  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedLinearlyDependent_Skimmed <- skim(DPA_ExcludedLinearlyDependent))

}

```

###  1.3.5 Pre-Processed Dataset
| Preliminary dataset assessment:
|
| **[A]** 333 rows (observations)
|      **[A.1]** Train Set = 267 observations
|      **[A.2]** Test Set = 66 observations
| 
| **[B]** 128 columns (variables)
|      **[B.1]** 1/128 response = <span style="color: #FF0000">Class</span> variable (factor)
|             **[B.1.1]** Levels = <span style="color: #FF0000">Class=Control</span> < <span style="color: #FF0000">Class=Impaired</span>
|      **[B.2]** 127/128 predictors = All remaining variables (3/127 factor + 124/127 numeric)
|
| **[C]** No pre-processing actions applied:
|      **[C.1]** No shape transformation applied since distributions were fairly normal
|      **[C.2]** Centering and scaling may be necessary due to the differences in ranges for the numeric variables however, these will be selectively applied based on model requirements
|      **[C.2]** No outlier treatment applied since the high values noted were minimal and contextually valid
|      **[C.3]** No predictors removed due to zero or near-zero variance 
|      **[C.4]** No predictors removed due to high correlation
|      **[C.5]** No predictors removed due to linear dependencies
|
```{r section_1.3.7, warning=FALSE, message=FALSE}
##################################
# Creating the pre-modelling
# train set
##################################
PMA_PreModelling_Train <- Alzheimer_Train
PMA_PreModelling_Train$Class <- as.factor(PMA_PreModelling_Train$Class)
PMA_PreModelling_Train$E2 <- as.factor(PMA_PreModelling_Train$E2)
PMA_PreModelling_Train$E3 <- as.factor(PMA_PreModelling_Train$E3)
PMA_PreModelling_Train$E4 <- as.factor(PMA_PreModelling_Train$E4)

##################################
# Gathering descriptive statistics
##################################
(PMA_PreModelling_Train_Skimmed <- skim(PMA_PreModelling_Train))

###################################
# Verifying the data dimensions
# for the train set
###################################
dim(PMA_PreModelling_Train)

##################################
# Formulating the test set
##################################
PMA_PreModelling_Test <- Alzheimer_Test
PMA_PreModelling_Test$Class <- as.factor(PMA_PreModelling_Test$Class)
PMA_PreModelling_Test$E2 <- as.factor(PMA_PreModelling_Test$E2)
PMA_PreModelling_Test$E3 <- as.factor(PMA_PreModelling_Test$E3)
PMA_PreModelling_Test$E4 <- as.factor(PMA_PreModelling_Test$E4)

##################################
# Gathering descriptive statistics
##################################
(PMA_PreModelling_Test_Skimmed <- skim(PMA_PreModelling_Test))

###################################
# Verifying the data dimensions
# for the test set
###################################
dim(PMA_PreModelling_Test)

```

##  1.4 Data Exploration
|
| Exploratory data analysis:
|
| **[A]** Numeric variables which demonstrated differential relationships with the <span style="color: #FF0000">Class</span> response variable include:
|      **[A.1]** <span style="color: #FF0000">Fibrinogen</span> variable (numeric)
|      **[A.2]** <span style="color: #FF0000">GRO_alpha</span> variable (numeric)
|      **[A.3]** <span style="color: #FF0000">FAS</span> variable (numeric)
|      **[A.4]** <span style="color: #FF0000">Eotaxin_3</span> variable (numeric)
|      **[A.5]** <span style="color: #FF0000">Creatine_Kinase_MB</span> variable (numeric)
|      **[A.6]** <span style="color: #FF0000">IGF_BP2</span> variable (numeric)
|      **[A.7]** <span style="color: #FF0000">Gamma_Interferon_Induced_Monokin</span> variable (numeric)
|      **[A.8]** <span style="color: #FF0000">MCP_2</span> variable (numeric)
|      **[A.9]** <span style="color: #FF0000">MIF</span> variable (numeric)
|      **[A.10]** <span style="color: #FF0000">Pancreatic_polypteptide</span> variable (numeric)
|      **[A.11]** <span style="color: #FF0000">PAI_1</span> variable (numeric)
|      **[A.12]** <span style="color: #FF0000">NT_proBNP</span> variable (numeric)
|      **[A.13]** <span style="color: #FF0000">MMP10</span> variable (numeric)
|      **[A.14]** <span style="color: #FF0000">MMP7</span> variable (numeric)
|      **[A.15]** <span style="color: #FF0000">TRAIL_R3</span> variable (numeric)
|      **[A.16]** <span style="color: #FF0000">Pulmonary_and_Activation_Regulat</span> variable (numeric)
|      **[A.17]** <span style="color: #FF0000">Resistin</span> variable (numeric)
|      **[A.18]** <span style="color: #FF0000">VEGF</span> variable (numeric)
|      **[A.19]** <span style="color: #FF0000">Thrombopoietin</span> variable (numeric)
|      **[A.20]** <span style="color: #FF0000">Thymus_Expressed_Chemokine_TECK</span> variable (numeric)
|
| **[B]** Factor variables which demonstrated relatively better differentiation of the <span style="color: #FF0000">Class</span> response variable between its <span style="color: #FF0000">Control</span> and <span style="color: #FF0000">Impaired</span> levels include:
|      **[B.1]** <span style="color: #FF0000">E2</span> variable (factor)
|      **[B.2]** <span style="color: #FF0000">E4</span> variable (factor)
|
```{r section_1.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
EDA <- PMA_PreModelling_Train

##################################
# Listing all predictors
##################################
EDA.Predictors <- EDA[,!names(EDA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
EDA.Predictors.Numeric <- EDA.Predictors[,sapply(EDA.Predictors, is.numeric)]
ncol(EDA.Predictors.Numeric)
names(EDA.Predictors.Numeric)

##################################
# Listing all factor predictors
##################################
EDA.Predictors.Factor <- EDA.Predictors[,sapply(EDA.Predictors, is.factor)]
ncol(EDA.Predictors.Factor)
names(EDA.Predictors.Factor)

##################################
# Formulating the box plots
##################################
featurePlot(x = EDA.Predictors.Numeric[1:124], 
            y = EDA$Class,
            plot = "box",
            scales = list(x = list(relation="free", rot = 90), 
                          y = list(relation="free")),
            adjust = 1.5, 
            pch = "|",
            layout=(c(4,4)))

##################################
# Restructuring the dataset for
# for barchart analysis
##################################
EDA.Bar.Source <- as.data.frame(cbind(EDA$Class,
                                      EDA.Predictors.Factor))
names(EDA.Bar.Source) <- c("Class",names(EDA.Predictors.Factor))
ncol(EDA.Bar.Source)

##################################
# Creating a function to formulate
# the proportions table
##################################
EDA.PropTable.Function <- function(FactorVar) {
  EDA.Bar.Source.FactorVar <- EDA.Bar.Source[,c("Class",
                                                FactorVar)]
  EDA.Bar.Source.FactorVar.Prop <- as.data.frame(prop.table(table(EDA.Bar.Source.FactorVar), 2))
  names(EDA.Bar.Source.FactorVar.Prop)[2] <- "Class"
  EDA.Bar.Source.FactorVar.Prop$Variable <- rep(FactorVar,nrow(EDA.Bar.Source.FactorVar.Prop))

  return(EDA.Bar.Source.FactorVar.Prop)

}

EDA.Bar.Source.FactorVar.Prop <- rbind(EDA.PropTable.Function("E2"),
                                       EDA.PropTable.Function("E3"),
                                       EDA.PropTable.Function("E4"))

(EDA.Barchart.FactorVar <- barchart(EDA.Bar.Source.FactorVar.Prop[,3] ~
                                      EDA.Bar.Source.FactorVar.Prop[,2] | EDA.Bar.Source.FactorVar.Prop[,4],
                                      data=EDA.Bar.Source.FactorVar.Prop,
                                      groups = EDA.Bar.Source.FactorVar.Prop[,1],
                                      stack=TRUE,
                                      ylab = "Proportion",
                                      xlab = "Class",
                                      auto.key = list(adj = 1),
                                      layout=(c(3,1))))

```

##  1.5 Simulated Annealing (SA) and Genetic Algorithms (GA)

###  1.5.1 Random Forest Without SA and GA (RF_FULL)
|
| **[A]** The random forest model from the  <mark style="background-color: #CCECFF">**randomForest**</mark> package was implemented without simulated annealing and genetic algorithms through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 1 hyperparameter:
|      **[B.1]** <span style="color: #FF0000">mtry</span> = number of randomly selected predictors held constant at a value of 11
|
| **[C]** The cross-validated model performance of the final model is summarized as follows:
|      **[C.1]** Final model configuration involves mtry=11
|      **[C.2]** Accuracy = 0.77975
|
| **[D]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[D.1]** <span style="color: #FF0000">MMP10</span> variable (numeric)
|      **[D.2]** <span style="color: #FF0000">Crystatin_C</span> variable (numeric)
|      **[D.3]** <span style="color: #FF0000">TRAIL_R3</span> variable (numeric)
|      **[D.4]** <span style="color: #FF0000">VEGF</span> variable (numeric)
|      **[D.5]** <span style="color: #FF0000">MCP_2</span> variable (numeric)
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** Accuracy = 0.80303
|
```{r section_1.5.1, warning=FALSE, message=FALSE}
##################################
# Converting all predictors to numeric
# for both train and test data
##################################
for (i in 1:ncol(PMA_PreModelling_Train)){
  if (names(PMA_PreModelling_Train)[i]!="Class"){
    PMA_PreModelling_Train[,i] <- as.numeric(PMA_PreModelling_Train[,i])
  }
}
summary(PMA_PreModelling_Train)

for (i in 1:ncol(PMA_PreModelling_Test)){
  if (names(PMA_PreModelling_Test)[i]!="Class"){
    PMA_PreModelling_Test[,i] <- as.numeric(PMA_PreModelling_Test[,i])
  }
}
summary(PMA_PreModelling_Test)

##################################
# Creating consistent fold assignments
# for the Cross Validation process
##################################
set.seed(12345678)
KFold_Indices <- createFolds(PMA_PreModelling_Train$Class ,
                             k = 10,
                             returnTrain=TRUE)

##################################
# Formulating a function to summarize
# model performance metrics
##################################
FiveMetricsSummary <- function(...) c(twoClassSummary(...), defaultSummary(...))

##################################
# Formulating the controls for the
# model training process
##################################
KFold_TrainControl <- trainControl(method = "cv",
                                   summaryFunction = FiveMetricsSummary,
                                   classProbs = TRUE,
                                   index = KFold_Indices)

##################################
# Running the random forest model
# by setting the caret method to 'rf'
##################################
set.seed(12345678)
RF_FULL_Tune <- caret::train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                        y = PMA_PreModelling_Train$Class,
                        method = "rf",
                        metric = "Accuracy",
                        tuneGrid = data.frame(mtry = floor(sqrt(length(names(PMA_PreModelling_Train) %in% c("Class"))))),
                        ntree = 50,
                        trControl = KFold_TrainControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
RF_FULL_Tune

RF_FULL_Tune$finalModel

RF_FULL_Tune$results

(RF_FULL_Train_Accuracy <- RF_FULL_Tune$results[,c("Accuracy")])

##################################
# Identifying and plotting the
# best model predictors
##################################
RF_FULL_VarImp <- varImp(RF_FULL_Tune, scale = TRUE)
plot(RF_FULL_VarImp,
     top=25,
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Random Forest",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
RF_FULL_Test <- data.frame(RF_FULL_Observed = PMA_PreModelling_Test$Class,
                      RF_FULL_Predicted = predict(RF_FULL_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "raw"))

RF_FULL_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
(RF_FULL_Test_Accuracy <- Accuracy(y_pred = RF_FULL_Test$RF_FULL_Predicted,
                                   y_true = RF_FULL_Test$RF_FULL_Observed))

```

###  1.5.2 Random Forest With Simulated Annealing (RF_SA)
|
| [Simulated Annealing](https://www.sciencedirect.com/science/article/pii/B9780080515816500593) is a global search method that makes small random changes or perturbations to an initial candidate solution. If the performance value for the perturbed value is better than the previous solution, the new solution is accepted. If not, an acceptance probability is determined based on the difference between the two performance values and the current iteration of the search. From this, a sub-optimal solution can be accepted on the off-change that it may eventually produce a better solution in subsequent iterations. In the context of feature selection, a solution is a binary vector that describes the current subset. The subset is perturbed by randomly changing a small number of members in the subset.
|
| **[A]** The random forest model from the  <mark style="background-color: #CCECFF">**randomForest**</mark> package was implemented with simulated annealing through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 1 hyperparameter:
|      **[B.1]** <span style="color: #FF0000">mtry</span> = number of randomly selected predictors held constant at a value of 5
|
| **[C]** Simulated annealing was applied with results as follows:
|      **[C.1]** 30 predictors were selected using the training data at iteration 8.
|      **[C.2]** Resampling showed that approximately 28 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=23 to 33
|      **[D.2]** Accuracy = 0.77618
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** Accuracy = 0.77272
|
```{r section_1.5.2, warning=FALSE, message=FALSE}
##################################
# Formulating the controls for the
# simulated annealing process
##################################
KFold_SAControl <- safsControl(method = "cv",
                      verbose = TRUE,
                      functions = rfSA,
                      index = KFold_Indices,
                      returnResamp = "final",
                      improve = 25)

##################################
# Running the random forest model
# by setting the caret method to 'rf'
# with implementation of simulated annealing
##################################
set.seed(12345678)
RF_SA_Tune <- caret::safs(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                          y = PMA_PreModelling_Train$Class,
                          iters = 10,
                          ntree = 50,
                          safsControl = KFold_SAControl)


##################################
# Reporting the cross-validation results
# for the train set
##################################
RF_SA_Tune

RF_SA_Tune$fit

RF_SA_Tune$averages

(RF_SA_Train_Accuracy <- RF_SA_Tune$averages[RF_SA_Tune$averages$Accuracy==max(RF_SA_Tune$averages$Accuracy),
                                               c("Accuracy")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
RF_SA_Test <- data.frame(RF_SA_Observed = PMA_PreModelling_Test$Class,
                         RF_SA_Predicted = predict(RF_SA_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "raw"))

RF_SA_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
(RF_SA_Test_Accuracy <- Accuracy(y_pred = RF_SA_Test$RF_SA_Predicted.pred,
                                 y_true = RF_SA_Test$RF_SA_Observed))

```

###  1.5.3 Random Forest With Genetic Algorithms (RF_GA)
|
| **[A]** The random forest model from the  <mark style="background-color: #CCECFF">**randomForest**</mark> package was implemented with genetic algorithms through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 1 hyperparameter:
|      **[B.1]** <span style="color: #FF0000">mtry</span> = number of randomly selected predictors held constant at a value of 9
|
| **[C]** Genetic algorithms was applied with results as follows:
|      **[C.1]** 93 predictors were selected using the training data at iteration 1.
|      **[C.2]** Resampling showed that approximately 63 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=45 to 101
|      **[D.2]** Accuracy = 0.79842
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** Accuracy = 0.78787
|
```{r section_1.5.3, warning=FALSE, message=FALSE}
##################################
# Formulating the controls for the
# genetic algorithms process
##################################
KFold_GAControl <- gafsControl(method = "cv",
                      verbose = TRUE,
                      functions = rfGA,
                      index = KFold_Indices,
                      returnResamp = "final")

##################################
# Running the random forest model
# by setting the caret method to 'rf'
# with implementation of genetic algorithms
##################################
set.seed(12345678)
RF_GA_Tune <- caret::gafs(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                          y = PMA_PreModelling_Train$Class,
                          iters = 10,
                          ntree = 50,
                          gafsControl = KFold_GAControl)


##################################
# Reporting the cross-validation results
# for the train set
##################################
RF_GA_Tune

RF_GA_Tune$fit

RF_GA_Tune$averages

(RF_GA_Train_Accuracy <- RF_GA_Tune$averages[RF_GA_Tune$averages$Accuracy==max(RF_GA_Tune$averages$Accuracy),
                                               c("Accuracy")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
RF_GA_Test <- data.frame(RF_GA_Observed = PMA_PreModelling_Test$Class,
                         RF_GA_Predicted = predict(RF_GA_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "raw"))

RF_GA_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
(RF_GA_Test_Accuracy <- Accuracy(y_pred = RF_GA_Test$RF_GA_Predicted.pred,
                                 y_true = RF_GA_Test$RF_GA_Observed))
```

###  1.5.4 Linear Discriminant Analysis Without SA and GA (LDA_FULL)
|
| **[A]** The linear discriminant analysis model from the  <mark style="background-color: #CCECFF">**MASS**</mark> package was implemented without simulated annealing and genetic algorithms through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** The cross-validated model performance of the final model is summarized as follows:
|      **[C.1]** Final model configuration is fixed due to the absence of a hyperparameter
|      **[C.2]** Accuracy = 0.76754
|
| **[D]** The model does not allow for ranking of predictors in terms of variable importance.
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** Accuracy = 0.75757
|
```{r section_1.5.4, warning=FALSE, message=FALSE}
##################################
# Converting all predictors to numeric
# for both train and test data
##################################
for (i in 1:ncol(PMA_PreModelling_Train)){
  if (names(PMA_PreModelling_Train)[i]!="Class"){
    PMA_PreModelling_Train[,i] <- as.numeric(PMA_PreModelling_Train[,i])
  }
}
summary(PMA_PreModelling_Train)

for (i in 1:ncol(PMA_PreModelling_Test)){
  if (names(PMA_PreModelling_Test)[i]!="Class"){
    PMA_PreModelling_Test[,i] <- as.numeric(PMA_PreModelling_Test[,i])
  }
}
summary(PMA_PreModelling_Test)

##################################
# Creating consistent fold assignments 
# for the Cross Validation process
##################################
set.seed(12345678)
KFold_Indices <- createFolds(PMA_PreModelling_Train$Class ,
                             k = 10,
                             returnTrain=TRUE)

##################################
# Formulating a function to summarize
# model performance metrics
##################################
FiveMetricsSummary <- function(...) c(twoClassSummary(...), defaultSummary(...))

##################################
# Formulating the controls for the 
# model training process
##################################
KFold_TrainControl <- trainControl(method = "cv",
                                   summaryFunction = FiveMetricsSummary,
                                   classProbs = TRUE,
                                   index = KFold_Indices)

##################################
# Running the linear discriminant analysis model
# by setting the caret method to 'lda'
##################################
set.seed(12345678)
LDA_FULL_Tune <- caret::train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                         y = PMA_PreModelling_Train$Class,
                         method = "lda",
                         metric = "Accuracy",
                         tol = 1.0e-12,
                         trControl = KFold_TrainControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
LDA_FULL_Tune

LDA_FULL_Tune$finalModel

LDA_FULL_Tune$results

(LDA_FULL_Train_Accuracy <- LDA_FULL_Tune$results[,c("Accuracy")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
LDA_FULL_Test <- data.frame(LDA_FULL_Observed = PMA_PreModelling_Test$Class,
                      LDA_FULL_Predicted = predict(LDA_FULL_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "raw"))

LDA_FULL_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
(LDA_FULL_Test_Accuracy <- Accuracy(y_pred = LDA_FULL_Test$LDA_FULL_Predicted,
                                    y_true = LDA_FULL_Test$LDA_FULL_Observed))

```

###  1.5.5 Linear Discriminant Analysis With Simulated Annealing (LDA_SA)
|
| [Simulated Annealing](https://www.sciencedirect.com/science/article/pii/B9780080515816500593) is a global search method that makes small random changes or perturbations to an initial candidate solution. If the performance value for the perturbed value is better than the previous solution, the new solution is accepted. If not, an acceptance probability is determined based on the difference between the two performance values and the current iteration of the search. From this, a sub-optimal solution can be accepted on the off-change that it may eventually produce a better solution in subsequent iterations. In the context of feature selection, a solution is a binary vector that describes the current subset. The subset is perturbed by randomly changing a small number of members in the subset.
|
| **[A]** The linear discriminant analysis model from the  <mark style="background-color: #CCECFF">**MASS**</mark> package was implemented with simulated annealing through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** Simulated annealing was applied with results as follows:
|      **[C.1]** 31 predictors were selected using the training data at iteration 9.
|      **[C.2]** Resampling showed that approximately 29 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=26 to 34
|      **[D.2]** Accuracy = 0.80941
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** Accuracy = 0.81818
|
```{r section_1.5.5, warning=FALSE, message=FALSE}
##################################
# Creating consistent fold assignments
# for the Cross Validation process
##################################
set.seed(12345678)
KFold_Indices <- createFolds(PMA_PreModelling_Train$Class ,
                             k = 10,
                             returnTrain=TRUE)

##################################
# Formulating the controls for the
# model training process
##################################
KFold_TrainControl <- trainControl(method = "cv",
                                   classProbs = TRUE)

##################################
# Formulating the controls for the
# simulated annealing process
##################################
KFold_SAControl <- safsControl(method = "cv",
                      verbose = TRUE,
                      functions = caretSA,
                      index = KFold_Indices,
                      returnResamp = "final")

##################################
# Running the linear discriminant analysis model
# by setting the caret method to 'lda'
# with implementation of simulated annealing
##################################
set.seed(12345678)
LDA_SA_Tune <- caret::safs(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                          y = PMA_PreModelling_Train$Class,
                          iters = 10,
                          method = "lda",
                          metric = "Accuracy",
                          safsControl = KFold_SAControl,
                          trControl = KFold_TrainControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
LDA_SA_Tune

LDA_SA_Tune$fit

LDA_SA_Tune$averages

(LDA_SA_Train_Accuracy <- LDA_SA_Tune$averages[LDA_SA_Tune$averages$Accuracy==max(LDA_SA_Tune$averages$Accuracy),
                                               c("Accuracy")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
LDA_SA_Test <- data.frame(LDA_SA_Observed = PMA_PreModelling_Test$Class,
                         LDA_SA_Predicted = predict(LDA_SA_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "raw"))

LDA_SA_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
(LDA_SA_Test_Accuracy <- Accuracy(y_pred = LDA_SA_Test$LDA_SA_Predicted.pred,
                                  y_true = LDA_SA_Test$LDA_SA_Observed))

```

###  1.5.6 Linear Discriminant Analysis With Genetic Algorithms (LDA_GA)
|
| **[A]** The linear discriminant analysis model from the  <mark style="background-color: #CCECFF">**MASS**</mark> package was implemented with genetic algorithms through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** Genetic algorithms was applied with results as follows:
|      **[C.1]** 61 predictors were selected using the training data at iteration 4.
|      **[C.2]** Resampling showed that approximately 47 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=30 to 62
|      **[D.2]** Accuracy = 0.79402
|
| **[E]** The independent test model peLDAormance of the final model is summarized as follows:
|      **[E.1]** Accuracy = 0.84848
|
```{r section_1.5.6, warning=FALSE, message=FALSE}
##################################
# Creating consistent fold assignments 
# for the Cross Validation process
##################################
set.seed(12345678)
KFold_Indices <- createFolds(PMA_PreModelling_Train$Class ,
                             k = 10,
                             returnTrain=TRUE)

##################################
# Formulating the controls for the 
# model training process
##################################
KFold_TrainControl <- trainControl(method = "cv",
                                   classProbs = TRUE)

##################################
# Formulating the controls for the
# genetic algorithms process
##################################
KFold_GAControl <- gafsControl(method = "cv",
                      verbose = TRUE,
                      functions = caretGA,
                      index = KFold_Indices,
                      returnResamp = "final")

##################################
# Running the linear discriminant analysis model
# by setting the caret method to 'lda'
# with implementation of genetic algorithms
##################################
set.seed(12345678)
LDA_GA_Tune <- caret::gafs(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                          y = PMA_PreModelling_Train$Class,
                          iters = 10,
                          method = "lda",
                          metric = "Accuracy",
                          gafsControl = KFold_GAControl,
                          trControl = KFold_TrainControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
LDA_GA_Tune

LDA_GA_Tune$fit

LDA_GA_Tune$averages

(LDA_GA_Train_Accuracy <- LDA_GA_Tune$averages[LDA_GA_Tune$averages$Accuracy==max(LDA_GA_Tune$averages$Accuracy),
                                               c("Accuracy")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
LDA_GA_Test <- data.frame(LDA_GA_Observed = PMA_PreModelling_Test$Class,
                         LDA_GA_Predicted = predict(LDA_GA_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "raw"))

LDA_GA_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
(LDA_GA_Test_Accuracy <- Accuracy(y_pred = LDA_GA_Test$LDA_GA_Predicted.pred,
                                  y_true = LDA_GA_Test$LDA_GA_Observed))

```

##  1.6 Model Evaluation Summary
|
| Model performance comparison:
|
| **[A]** The simulated annealing and genetic algorithms to select a subset of informative predictors performed better for models without an inherent variable selection feature (i.e. linear discriminant analysis as compared to random forest). 
|      **[A.1]** RF: Random Forest (<mark style="background-color: #CCECFF">**randomForest**</mark> package)
|             **[A.1.1]** RF_FULL: Cross-Validation Accuracy = 0.77975, Test Accuracy = 0.80303 
|             **[A.1.2]** RF_SA: Cross-Validation Accuracy = 0.77618, Test Accuracy = 0.77272  
|             **[A.1.3]** RF_GA: Cross-Validation Accuracy = 0.79842, Test Accuracy = 0.78787
|      **[A.2]** LDA: Linear Discriminant Analysis (<mark style="background-color: #CCECFF">**MASS**</mark> package)
|             **[A.2.1]** LDA_FULL: Cross-Validation Accuracy = 0.76754, Test Accuracy = 0.75757 
|             **[A.2.2]** LDA_SA: Cross-Validation Accuracy = 0.80941, Test Accuracy = 0.81818  
|             **[A.2.3]** LDA_GA: Cross-Validation Accuracy = 0.79402, Test Accuracy = 0.84848
|
```{r section_1.6, warning=FALSE, message=FALSE}
##################################
# Consolidating all evaluation results
# for the train and test sets
# using the accuracy metric
##################################
Model <- c('RF_FULL','RF_SA','RF_GA','LDA_FULL','LDA_SA','LDA_GA',
           'RF_FULL','RF_SA','RF_GA','LDA_FULL','LDA_SA','LDA_GA')

Set <- c(rep('Cross-Validation',6),rep('Test',6))

Accuracy <- c(RF_FULL_Train_Accuracy,
              RF_SA_Train_Accuracy,
              RF_GA_Train_Accuracy,
              LDA_FULL_Train_Accuracy,
              LDA_SA_Train_Accuracy,
              LDA_GA_Train_Accuracy,
              RF_FULL_Test_Accuracy,
              RF_SA_Test_Accuracy,
              RF_GA_Test_Accuracy,
              LDA_FULL_Test_Accuracy,
              LDA_SA_Test_Accuracy,
              LDA_GA_Test_Accuracy)

Accuracy_Summary <- as.data.frame(cbind(Model,Set,Accuracy))

Accuracy_Summary$Accuracy <- as.numeric(as.character(Accuracy_Summary$Accuracy))
Accuracy_Summary$Set <- factor(Accuracy_Summary$Set,
                               levels = c("Cross-Validation",
                                                   "Test"))
Accuracy_Summary$Model <- factor(Accuracy_Summary$Model,
                                 levels = c('RF_FULL',
                                            'RF_SA',
                                            'RF_GA',
                                            'LDA_FULL',
                                            'LDA_SA',
                                            'LDA_GA'))

print(Accuracy_Summary, row.names=FALSE)

(Accuracy_Plot <- dotplot(Model ~ Accuracy,
                          data = Accuracy_Summary,
                          groups = Set,
                          main = "Classification Model Performance Comparison",
                          ylab = "Model",
                          xlab = "Accuracy",
                          auto.key = list(adj = 1),
                          type=c("p", "h"),
                          origin = 0,
                          alpha = 0.45,
                          pch = 16,
                          cex = 2))

```

# **2. References**
|
| **[Book]** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson
| **[Book]** [An Introduction to Statistical Learning](https://www.statlearning.com/) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani
| **[Book]** [Multivariate Data Visualization with R](http://lmdvr.r-forge.r-project.org/figures/figures.html) by Deepayan Sarkar
| **[Book]** [Machine Learning](https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/) by Samuel Jackson
| **[Book]** [Data Modeling Methods](https://bookdown.org/larget_jacob/data-modeling-methods/) by Jacob Larget
| **[Book]** [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/index.html) by Max Kuhn and Kjell Johnson
| **[R Package]** [AppliedPredictiveModeling](https://cran.r-project.org/web//packages/AppliedPredictiveModeling/AppliedPredictiveModeling.pdf) by Max Kuhn
| **[R Package]** [caret](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[R Package]** [rpart](https://mran.microsoft.com/web/packages/rpart/rpart.pdf) by Terry Therneau and Beth Atkinson
| **[R Package]** [lattice](https://cran.r-project.org/web/packages/lattice/lattice.pdf) by  Deepayan Sarkar
| **[R Package]** [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html/) by Hadley Wickham
| **[R Package]** [moments](https://cran.r-project.org/web/packages/moments/index.html) by Lukasz Komsta and Frederick
| **[R Package]** [skimr](https://cran.r-project.org/web/packages/skimr/skimr.pdf) by  Elin Waring
| **[R Package]** [RANN](https://cran.r-project.org/web/packages/RANN/RANN.pdf) by  Sunil Arya, David Mount, Samuel Kemp and Gregory Jefferis
| **[R Package]** [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) by Taiyun Wei
| **[R Package]** [tidyverse](https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf) by Hadley Wickham
| **[R Package]** [lares](https://cran.rstudio.com/web/packages/lares/lares.pdf) by Bernardo Lares
| **[R Package]** [DMwR](https://mran.microsoft.com/snapshot/2016-05-02/web/packages/DMwR/DMwR.pdf) by Luis Torgo
| **[R Package]** [gridExtra](https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf) by Baptiste Auguie and Anton Antonov
| **[R Package]** [rattle](https://cran.r-project.org/web/packages/rattle/rattle.pdf) by Graham Williams
| **[R Package]** [rpart.plot](https://cran.r-project.org/web/packages/rpart.plot/rpart.plot.pdf) by Stephen Milborrow
| **[R Package]** [RColorBrewer](https://cran.r-project.org/web//packages/RColorBrewer/RColorBrewer.pdf) by Erich Neuwirth
| **[R Package]** [stats](https://search.r-project.org/R/refmans/stats/html/00Index.html) by R Core Team
| **[R Package]** [pls](https://cran.r-project.org/web/packages/pls/pls.pdf) by Kristian Hovde Liland
| **[R Package]** [nnet](https://cran.r-project.org/web/packages/nnet/nnet.pdf) by Brian Ripley
| **[R Package]** [elasticnet](https://cran.r-project.org/web/packages/elasticnet/elasticnet.pdf) by Hui Zou
| **[R Package]** [earth](https://cran.r-project.org/web/packages/earth/earth.pdf) by Stephen Milborrow
| **[R Package]** [party](https://cran.r-project.org/web/packages/party/party.pdf) by Torsten Hothorn
| **[R Package]** [kernlab](https://cran.r-project.org/web/packages/kernlab/kernlab.pdf) by Alexandros Karatzoglou
| **[R Package]** [randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf) by Andy Liaw
| **[R Package]** [pROC](https://cran.r-project.org/web/packages/pROC/pROC.pdf) by Xavier Robin
| **[R Package]** [mda](https://cran.r-project.org/web/packages/mda/mda.pdf) by Trevor Hastie
| **[R Package]** [klaR](https://cran.r-project.org/web/packages/klaR/klaR.pdf) by Christian Roever, Nils Raabe, Karsten Luebke, Uwe Ligges, Gero Szepannek, Marc Zentgraf and David Meyer
| **[R Package]** [pamr](https://cran.r-project.org/web/packages/pamr/pamr.pdf) by Trevor Hastie, Rob Tibshirani, Balasubramanian Narasimhan and Gil Chu
| **[Article]** [The caret Package](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[Article]** [A Short Introduction to the caret Package](https://cran.r-project.org/web/packages/caret/vignettes/caret.html) by Max Kuhn
| **[Article]** [Caret Package  A Practical Guide to Machine Learning in R](https://www.machinelearningplus.com/machine-learning/caret-package/#:~:text=Caret%20is%20short%20for%20Classification%20And%20REgression%20Training.,track%20of%20which%20algorithm%20resides%20in%20which%20package.) by Selva Prabhakaran
| **[Article]** [Tuning Machine Learning Models Using the Caret R Package](https://machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/) by Jason Brownlee
| **[Article]** [Lattice Graphs](http://www.sthda.com/english/wiki/lattice-graphs) by Alboukadel Kassambara
| **[Article]** [A Tour of Machine Learning Algorithms](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/) by Jason Brownlee
| **[Article]** [How to Choose a Feature Selection Method For Machine Learning](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/) by Jason Brownlee
| **[Article]** [Simulated Annealing Feature Selection](https://www.r-bloggers.com/2015/01/simulated-annealing-feature-selection/) by Max Kuhn
| **[Article]** [Simulated Annealing Algorithm Explained from Scratch (Python)](https://www.machinelearningplus.com/machine-learning/simulated-annealing-algorithm-explained-from-scratch-python/) by Naveen James
| **[Article]** [Simulated Annealing: A Simple Overview in 5 Points](https://www.jigsawacademy.com/blogs/ai-ml/simulated-annealing/) by Ajay Ohri
| **[Article]** [Simulated Annealing From Scratch in Python](https://machinelearningmastery.com/simulated-annealing-from-scratch-in-python/#:~:text=Simulated%20Annealing%20is%20a%20stochastic%20global%20search%20optimization,other%20local%20search%20algorithms%20do%20not%20operate%20well.) by Jason Brownlee
| **[Article]** [Simulated Annealing for Beginners](https://www.theprojectspot.com/tutorial-post/simulated-annealing-algorithm-for-beginners/6) by Lee Jacobson
| **[Article]** [Genetic Algorithms for Feature Selection](https://neuraldesigner.com/blog/genetic_algorithms_for_feature_selection#:~:text=One%20of%20the%20most%20advanced%20algorithms%20for%20feature,successive%20generations%20to%20better%20adapt%20to%20the%20environment.) by By Fernando Gomez, Alberto Quesada, Pedro ngel Fraile Manzano and Roberto Lopez
| **[Article]** [Feature Selection Using Genetic Algorithm](https://www.letthedataconfess.com/blog/2020/01/08/feature-selection-using-genetic-algorithm-2/) by Let The Data Confess Team
| **[Article]** [Feature Selection using Genetic Algorithms in R](https://www.r-bloggers.com/2019/01/feature-selection-using-genetic-algorithms-in-r/) by Pablo Casas
| **[Article]** [What is Genetic Algorithm?](https://www.educba.com/what-is-genetic-algorithm/) by Priya Pedamkar
| **[Article]** [Genetic Algorithms](https://www.geeksforgeeks.org/genetic-algorithms/) by Geeks For Geeks Team
| **[Article]** [Decision Tree Algorithm Examples In Data Mining](https://www.softwaretestinghelp.com/decision-tree-algorithm-examples-data-mining/) by Software Testing Help Team
| **[Article]** [4 Types of Classification Tasks in Machine Learning](https://machinelearningmastery.com/types-of-classification-in-machine-learning/) by Jason Brownlee
| **[Article]** [Spot-Check Classification Machine Learning Algorithms in Python with scikit-learn](https://machinelearningmastery.com/spot-check-classification-machine-learning-algorithms-python-scikit-learn/) by Jason Brownlee
| **[Article]** [An Introduction to Naive Bayes Algorithm for Beginners](https://www.turing.com/kb/an-introduction-to-naive-bayes-algorithm-for-beginners) by Turing Team
| **[Article]** [Machine Learning Tutorial: A Step-by-Step Guide for Beginners](http://www.feat.engineering/index.html) by Mayank Banoula
| **[Article]** [Discriminant Analysis Essentials in R](http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/) by Alboukadel Kassambara
| **[Article]** [Linear Discriminant Analysis, Explained](https://yangxiaozhou.github.io/data/2019/10/02/linear-discriminant-analysis.html) by Xiaozhou Yang
| **[Article]** [Classification Tree](https://support.bccvl.org.au/support/solutions/articles/6000083204-classification-tree) by BCCVL Team
| **[Article]** [Random Forest](https://support.bccvl.org.au/support/solutions/articles/6000083217-random-forest) by BCCVL Team
| **[Article]** [Generalized Linear Model](https://support.bccvl.org.au/support/solutions/articles/6000083213-generalized-linear-model) by BCCVL Team
| **[Publication]** [Optimization by Simulated Annealing](https://www.sciencedirect.com/science/article/pii/B9780080515816500593) by S Kirkpatrick, C Gelatt and M Vecchi (Science)
| **[Publication]** [Simulated Annealing Algorithms: An Overview](https://ieeexplore.ieee.org/document/17235) by R Rutenbar (IEEE Circuits and Devices Magazine)
| **[Publication]** [An Introduction to Genetic Algorithms](http://mitpress.mit.edu/9780262133166/) by Melanie Mitchell (Complex Adaptive Systems)
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
| **[Course]** [Regression Methods](https://online.stat.psu.edu/stat501/) by Penn State Eberly College of Science
| **[Course]** [Applied Regression Analysis](https://online.stat.psu.edu/stat462/) by Penn State Eberly College of Science
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
|
|
|
|